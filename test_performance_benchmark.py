#!/usr/bin/env python3
"""
å²è±å§†æ¼‚æµæ€§èƒ½åŸºå‡†æµ‹è¯•

æµ‹è¯•ä¼˜åŒ–åçš„çŠ¶æ€æ¨¡å¼æ¶æ„ç›¸æ¯”åŸå§‹æ¶æ„çš„æ€§èƒ½æ”¹è¿›
é‡ç‚¹æµ‹è¯•ï¼šç¼“å­˜æ•ˆæœã€APIè°ƒç”¨ä¼˜åŒ–ã€å†…å­˜ä½¿ç”¨ã€å“åº”æ—¶é—´
"""

import sys
import time
import os
import gc
import psutil
import traceback
from datetime import datetime
import json

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = os.path.dirname(os.path.abspath(__file__))
sys.path.insert(0, project_root)

class PerformanceBenchmark:
    """æ€§èƒ½åŸºå‡†æµ‹è¯•å·¥å…·"""
    
    def __init__(self):
        self.results = {}
        self.process = psutil.Process()
        
    def start_timer(self, operation_name):
        """å¼€å§‹è®¡æ—¶"""
        self.start_time = time.time()
        self.start_memory = self.process.memory_info().rss / 1024 / 1024  # MB
        print(f"â±ï¸  å¼€å§‹è®¡æ—¶: {operation_name}")
        
    def end_timer(self, operation_name):
        """ç»“æŸè®¡æ—¶å¹¶è®°å½•ç»“æœ"""
        end_time = time.time()
        end_memory = self.process.memory_info().rss / 1024 / 1024  # MB
        
        duration = end_time - self.start_time
        memory_diff = end_memory - self.start_memory
        
        self.results[operation_name] = {
            'duration': duration,
            'memory_usage': end_memory,
            'memory_diff': memory_diff,
            'timestamp': datetime.now().isoformat()
        }
        
        print(f"â° {operation_name} å®Œæˆ: {duration:.2f}ç§’, å†…å­˜: {end_memory:.1f}MB (+{memory_diff:.1f}MB)")
        return duration, memory_diff

def test_import_performance():
    """æµ‹è¯•æ¨¡å—å¯¼å…¥æ€§èƒ½"""
    print("\nğŸ“¦ æµ‹è¯•æ¨¡å—å¯¼å…¥æ€§èƒ½...")
    benchmark = PerformanceBenchmark()
    
    try:
        # æµ‹è¯•æ ¸å¿ƒç»„ä»¶å¯¼å…¥
        benchmark.start_timer("å¯¼å…¥æ ¸å¿ƒç»„ä»¶")
        from core.components.derive_state_machine import DeriveStateMachine
        from core.components.derive_context import DeriveContext
        from core.components.performance_optimizer import global_optimizer, global_resource_manager
        benchmark.end_timer("å¯¼å…¥æ ¸å¿ƒç»„ä»¶")
        
        # æµ‹è¯•çŠ¶æ€ç±»å¯¼å…¥
        benchmark.start_timer("å¯¼å…¥çŠ¶æ€ç±»")
        from core.components.states import (
            InitState, GenSlimeImageState, ShowSlimeImageState,
            ShowGreetingState, AskPhotoState, TakePhotoState,
            AnalyzePhotoState, SuggestDestinationState
        )
        benchmark.end_timer("å¯¼å…¥çŠ¶æ€ç±»")
        
        # æµ‹è¯•å·¥å…·ç±»å¯¼å…¥
        benchmark.start_timer("å¯¼å…¥å·¥å…·ç±»")
        from core.components.derive_utils import DeriveChatUtils, DeriveImageUtils
        benchmark.end_timer("å¯¼å…¥å·¥å…·ç±»")
        
        print("âœ… æ¨¡å—å¯¼å…¥æ€§èƒ½æµ‹è¯•å®Œæˆ")
        return True, benchmark.results
        
    except Exception as e:
        print(f"âŒ æ¨¡å—å¯¼å…¥æµ‹è¯•å¤±è´¥: {e}")
        return False, None

def test_state_machine_initialization():
    """æµ‹è¯•çŠ¶æ€æœºåˆå§‹åŒ–æ€§èƒ½"""
    print("\nğŸš€ æµ‹è¯•çŠ¶æ€æœºåˆå§‹åŒ–æ€§èƒ½...")
    benchmark = PerformanceBenchmark()
    
    try:
        from core.components.derive_state_machine import DeriveStateMachine
        
        # æµ‹è¯•åˆ›å»ºçŠ¶æ€æœº
        benchmark.start_timer("åˆ›å»ºçŠ¶æ€æœº")
        initial_text = "æ€§èƒ½æµ‹è¯•åˆå§‹æ–‡æœ¬"
        state_machine = DeriveStateMachine(initial_text)
        duration, memory_diff = benchmark.end_timer("åˆ›å»ºçŠ¶æ€æœº")
        
        # æµ‹è¯•çŠ¶æ€æ³¨å†Œ
        benchmark.start_timer("çŠ¶æ€æ³¨å†Œ")
        state_machine.initialize_states()
        benchmark.end_timer("çŠ¶æ€æ³¨å†Œ")
        
        # éªŒè¯çŠ¶æ€æ•°é‡
        state_count = len(state_machine.states)
        print(f"ğŸ“Š æ³¨å†Œäº† {state_count} ä¸ªçŠ¶æ€")
        
        # æ¸…ç†
        state_machine.context.cleanup()
        
        print("âœ… çŠ¶æ€æœºåˆå§‹åŒ–æ€§èƒ½æµ‹è¯•å®Œæˆ")
        return True, benchmark.results
        
    except Exception as e:
        print(f"âŒ çŠ¶æ€æœºåˆå§‹åŒ–æµ‹è¯•å¤±è´¥: {e}")
        return False, None

def test_cache_performance():
    """æµ‹è¯•ç¼“å­˜æ€§èƒ½"""
    print("\nğŸ’¾ æµ‹è¯•ç¼“å­˜æ€§èƒ½...")
    benchmark = PerformanceBenchmark()
    
    try:
        from core.components.performance_optimizer import global_optimizer
        
        # å‡†å¤‡æµ‹è¯•æ•°æ®
        test_data = {
            'large_string': 'x' * 10000,
            'complex_dict': {f'key_{i}': f'value_{i}' for i in range(1000)},
            'list_data': list(range(5000))
        }
        
        # æµ‹è¯•ç¼“å­˜å†™å…¥æ€§èƒ½
        benchmark.start_timer("ç¼“å­˜å†™å…¥")
        for key, value in test_data.items():
            cache_key = global_optimizer.cache_key(f"test_{key}", value)
            global_optimizer.set_cache(cache_key, value)
        benchmark.end_timer("ç¼“å­˜å†™å…¥")
        
        # æµ‹è¯•ç¼“å­˜è¯»å–æ€§èƒ½
        benchmark.start_timer("ç¼“å­˜è¯»å–")
        cache_hits = 0
        for key, value in test_data.items():
            cache_key = global_optimizer.cache_key(f"test_{key}", value)
            cached_value = global_optimizer.get_cache(cache_key)
            if cached_value is not None:
                cache_hits += 1
        benchmark.end_timer("ç¼“å­˜è¯»å–")
        
        print(f"ğŸ“Š ç¼“å­˜å‘½ä¸­ç‡: {cache_hits}/{len(test_data)} ({(cache_hits/len(test_data)*100):.1f}%)")
        
        # æ¸…ç†ç¼“å­˜
        global_optimizer.clear_cache("test_")
        
        print("âœ… ç¼“å­˜æ€§èƒ½æµ‹è¯•å®Œæˆ")
        return True, benchmark.results
        
    except Exception as e:
        print(f"âŒ ç¼“å­˜æ€§èƒ½æµ‹è¯•å¤±è´¥: {e}")
        return False, None

def test_resource_management_performance():
    """æµ‹è¯•èµ„æºç®¡ç†æ€§èƒ½"""
    print("\nğŸ§¹ æµ‹è¯•èµ„æºç®¡ç†æ€§èƒ½...")
    benchmark = PerformanceBenchmark()
    
    try:
        from core.components.performance_optimizer import global_resource_manager
        
        # åˆ›å»ºæµ‹è¯•èµ„æº
        class TestResource:
            def __init__(self, name):
                self.name = name
                self.data = list(range(1000))  # æ¨¡æ‹Ÿä¸€äº›æ•°æ®
            
            def cleanup(self):
                self.data = None
        
        # æµ‹è¯•èµ„æºè·å–æ€§èƒ½
        benchmark.start_timer("èµ„æºè·å–")
        resources = []
        for i in range(100):
            resource = TestResource(f"test_resource_{i}")
            global_resource_manager.acquire_resource(f"test_{i}", resource)
            resources.append(resource)
        benchmark.end_timer("èµ„æºè·å–")
        
        print(f"ğŸ“Š ç®¡ç†çš„èµ„æºæ•°é‡: {len(global_resource_manager.active_resources)}")
        
        # æµ‹è¯•èµ„æºé‡Šæ”¾æ€§èƒ½
        benchmark.start_timer("èµ„æºé‡Šæ”¾")
        global_resource_manager.release_all()
        benchmark.end_timer("èµ„æºé‡Šæ”¾")
        
        print(f"ğŸ“Š é‡Šæ”¾åå‰©ä½™èµ„æº: {len(global_resource_manager.active_resources)}")
        
        print("âœ… èµ„æºç®¡ç†æ€§èƒ½æµ‹è¯•å®Œæˆ")
        return True, benchmark.results
        
    except Exception as e:
        print(f"âŒ èµ„æºç®¡ç†æ€§èƒ½æµ‹è¯•å¤±è´¥: {e}")
        return False, None

def test_state_transition_performance():
    """æµ‹è¯•çŠ¶æ€è½¬æ¢æ€§èƒ½"""
    print("\nğŸ”„ æµ‹è¯•çŠ¶æ€è½¬æ¢æ€§èƒ½...")
    benchmark = PerformanceBenchmark()
    
    try:
        from core.components.derive_state_machine import DeriveStateMachine
        from core.components.derive_states import DeriveState
        
        # åˆ›å»ºçŠ¶æ€æœº
        initial_text = "çŠ¶æ€è½¬æ¢æ€§èƒ½æµ‹è¯•"
        state_machine = DeriveStateMachine(initial_text)
        state_machine.initialize_states()
        
        # æµ‹è¯•çŠ¶æ€è½¬æ¢æ€§èƒ½
        benchmark.start_timer("çŠ¶æ€è½¬æ¢")
        
        transition_count = 0
        test_states = [
            DeriveState.INIT,
            DeriveState.GEN_SLIME_IMAGE,
            DeriveState.SHOW_SLIME_IMAGE,
            DeriveState.SHOW_GREETING,
            DeriveState.CLEANUP
        ]
        
        for state in test_states:
            if state in state_machine.states:
                success = state_machine.transition_to(state)
                if success:
                    transition_count += 1
                time.sleep(0.01)  # çŸ­æš‚å»¶è¿Ÿ
        
        duration, memory_diff = benchmark.end_timer("çŠ¶æ€è½¬æ¢")
        
        print(f"ğŸ“Š å®Œæˆ {transition_count} æ¬¡çŠ¶æ€è½¬æ¢")
        print(f"ğŸ“Š å¹³å‡æ¯æ¬¡è½¬æ¢: {duration/transition_count:.3f}ç§’")
        
        # æ¸…ç†
        state_machine.context.cleanup()
        
        print("âœ… çŠ¶æ€è½¬æ¢æ€§èƒ½æµ‹è¯•å®Œæˆ")
        return True, benchmark.results
        
    except Exception as e:
        print(f"âŒ çŠ¶æ€è½¬æ¢æ€§èƒ½æµ‹è¯•å¤±è´¥: {e}")
        return False, None

def test_memory_usage_pattern():
    """æµ‹è¯•å†…å­˜ä½¿ç”¨æ¨¡å¼"""
    print("\nğŸ§  æµ‹è¯•å†…å­˜ä½¿ç”¨æ¨¡å¼...")
    benchmark = PerformanceBenchmark()
    
    try:
        from core.components.derive_state_machine import DeriveStateMachine
        
        # è®°å½•åˆå§‹å†…å­˜
        initial_memory = benchmark.process.memory_info().rss / 1024 / 1024
        print(f"ğŸ“Š åˆå§‹å†…å­˜ä½¿ç”¨: {initial_memory:.1f}MB")
        
        memory_samples = []
        
        # åˆ›å»ºå¤šä¸ªçŠ¶æ€æœºå®ä¾‹æµ‹è¯•å†…å­˜å¢é•¿
        for i in range(5):
            print(f"åˆ›å»ºç¬¬ {i+1} ä¸ªçŠ¶æ€æœºå®ä¾‹...")
            
            state_machine = DeriveStateMachine(f"æµ‹è¯•å®ä¾‹ {i+1}")
            state_machine.initialize_states()
            
            current_memory = benchmark.process.memory_info().rss / 1024 / 1024
            memory_samples.append(current_memory)
            print(f"  å†…å­˜ä½¿ç”¨: {current_memory:.1f}MB")
            
            # æ¸…ç†
            state_machine.context.cleanup()
            
            # å¼ºåˆ¶åƒåœ¾å›æ”¶
            gc.collect()
            
            after_cleanup = benchmark.process.memory_info().rss / 1024 / 1024
            print(f"  æ¸…ç†åå†…å­˜: {after_cleanup:.1f}MB")
        
        # åˆ†æå†…å­˜ä½¿ç”¨æ¨¡å¼
        max_memory = max(memory_samples)
        avg_memory = sum(memory_samples) / len(memory_samples)
        memory_growth = max_memory - initial_memory
        
        print(f"ğŸ“Š å†…å­˜ä½¿ç”¨åˆ†æ:")
        print(f"   æœ€å¤§å†…å­˜: {max_memory:.1f}MB")
        print(f"   å¹³å‡å†…å­˜: {avg_memory:.1f}MB")
        print(f"   å†…å­˜å¢é•¿: {memory_growth:.1f}MB")
        
        benchmark.results["å†…å­˜åˆ†æ"] = {
            'initial_memory': initial_memory,
            'max_memory': max_memory,
            'avg_memory': avg_memory,
            'memory_growth': memory_growth
        }
        
        print("âœ… å†…å­˜ä½¿ç”¨æ¨¡å¼æµ‹è¯•å®Œæˆ")
        return True, benchmark.results
        
    except Exception as e:
        print(f"âŒ å†…å­˜ä½¿ç”¨æ¨¡å¼æµ‹è¯•å¤±è´¥: {e}")
        return False, None

def test_error_recovery_performance():
    """æµ‹è¯•é”™è¯¯æ¢å¤æ€§èƒ½"""
    print("\nğŸš¨ æµ‹è¯•é”™è¯¯æ¢å¤æ€§èƒ½...")
    benchmark = PerformanceBenchmark()
    
    try:
        from core.components.performance_optimizer import global_optimizer
        
        # æµ‹è¯•æ™ºèƒ½é‡è¯•æ€§èƒ½
        benchmark.start_timer("é”™è¯¯æ¢å¤")
        
        retry_count = 0
        
        def sometimes_failing_function():
            nonlocal retry_count
            retry_count += 1
            if retry_count < 3:  # å‰ä¸¤æ¬¡å¤±è´¥
                raise Exception(f"æ¨¡æ‹Ÿé”™è¯¯ #{retry_count}")
            return f"æˆåŠŸï¼é‡è¯•äº† {retry_count} æ¬¡"
        
        try:
            result = global_optimizer.smart_retry(
                sometimes_failing_function,
                max_retries=5,
                base_delay=0.01,  # çŸ­å»¶è¿Ÿç”¨äºæµ‹è¯•
                operation_name="test_recovery"
            )
            print(f"ğŸ“Š é‡è¯•ç»“æœ: {result}")
        except Exception as e:
            print(f"ğŸ“Š é‡è¯•æœ€ç»ˆå¤±è´¥: {e}")
        
        duration, memory_diff = benchmark.end_timer("é”™è¯¯æ¢å¤")
        
        # æ£€æŸ¥å¤±è´¥ç»Ÿè®¡
        failure_count = global_optimizer.get_failure_count("test_recovery")
        print(f"ğŸ“Š è®°å½•çš„å¤±è´¥æ¬¡æ•°: {failure_count}")
        
        print("âœ… é”™è¯¯æ¢å¤æ€§èƒ½æµ‹è¯•å®Œæˆ")
        return True, benchmark.results
        
    except Exception as e:
        print(f"âŒ é”™è¯¯æ¢å¤æ€§èƒ½æµ‹è¯•å¤±è´¥: {e}")
        return False, None

def save_benchmark_results(all_results):
    """ä¿å­˜åŸºå‡†æµ‹è¯•ç»“æœ"""
    try:
        results_file = "performance_benchmark_results.json"
        
        # åˆå¹¶æ‰€æœ‰ç»“æœ
        combined_results = {
            'test_timestamp': datetime.now().isoformat(),
            'system_info': {
                'python_version': sys.version,
                'platform': sys.platform,
                'cpu_count': psutil.cpu_count(),
                'memory_total': psutil.virtual_memory().total / 1024 / 1024 / 1024  # GB
            },
            'test_results': {}
        }
        
        for test_name, results in all_results.items():
            if results:
                combined_results['test_results'][test_name] = results
        
        # ä¿å­˜åˆ°æ–‡ä»¶
        with open(results_file, 'w', encoding='utf-8') as f:
            json.dump(combined_results, f, indent=2, ensure_ascii=False)
        
        print(f"ğŸ“„ åŸºå‡†æµ‹è¯•ç»“æœå·²ä¿å­˜åˆ°: {results_file}")
        return True
        
    except Exception as e:
        print(f"âŒ ä¿å­˜åŸºå‡†æµ‹è¯•ç»“æœå¤±è´¥: {e}")
        return False

def main():
    """ä¸»æ€§èƒ½æµ‹è¯•å‡½æ•°"""
    print("ğŸ“Š å²è±å§†æ¼‚æµæ€§èƒ½åŸºå‡†æµ‹è¯•")
    print(f"ğŸ• æµ‹è¯•æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("ğŸ¯ æµ‹è¯•ç›®æ ‡: éªŒè¯ä¼˜åŒ–åç³»ç»Ÿçš„æ€§èƒ½æ”¹è¿›")
    print("=" * 60)
    
    # æ€§èƒ½æµ‹è¯•åˆ—è¡¨
    performance_tests = [
        ("æ¨¡å—å¯¼å…¥æ€§èƒ½", test_import_performance),
        ("çŠ¶æ€æœºåˆå§‹åŒ–æ€§èƒ½", test_state_machine_initialization),
        ("ç¼“å­˜æ€§èƒ½", test_cache_performance),
        ("èµ„æºç®¡ç†æ€§èƒ½", test_resource_management_performance),
        ("çŠ¶æ€è½¬æ¢æ€§èƒ½", test_state_transition_performance),
        ("å†…å­˜ä½¿ç”¨æ¨¡å¼", test_memory_usage_pattern),
        ("é”™è¯¯æ¢å¤æ€§èƒ½", test_error_recovery_performance)
    ]
    
    all_results = {}
    tests_passed = 0
    total_start_time = time.time()
    
    try:
        for test_name, test_func in performance_tests:
            print(f"\n{'='*20} {test_name} {'='*20}")
            
            try:
                success, results = test_func()
                if success:
                    tests_passed += 1
                    all_results[test_name] = results
                    print(f"âœ… {test_name} å®Œæˆ")
                else:
                    print(f"âŒ {test_name} å¤±è´¥")
                    all_results[test_name] = None
            
            except Exception as e:
                print(f"ğŸ’¥ {test_name} å‡ºç°å¼‚å¸¸: {e}")
                all_results[test_name] = None
            
            # æ¯ä¸ªæµ‹è¯•åè¿›è¡Œåƒåœ¾å›æ”¶
            gc.collect()
            time.sleep(0.5)
    
    except KeyboardInterrupt:
        print("\nâ¹ï¸  æ€§èƒ½æµ‹è¯•è¢«ç”¨æˆ·ä¸­æ–­")
    
    # è®¡ç®—æ€»è€—æ—¶
    total_duration = time.time() - total_start_time
    
    # è¾“å‡ºæ±‡æ€»ç»“æœ
    print("\n" + "=" * 60)
    print("ğŸ“Š æ€§èƒ½åŸºå‡†æµ‹è¯•ç»“æœæ±‡æ€»")
    print(f"âœ… é€šè¿‡æµ‹è¯•: {tests_passed}/{len(performance_tests)}")
    print(f"â±ï¸  æ€»æµ‹è¯•æ—¶é—´: {total_duration:.2f}ç§’")
    print(f"ğŸ“ˆ æˆåŠŸç‡: {(tests_passed/len(performance_tests))*100:.1f}%")
    
    # ä¿å­˜è¯¦ç»†ç»“æœ
    save_benchmark_results(all_results)
    
    # è¾“å‡ºå…³é”®æ€§èƒ½æŒ‡æ ‡
    print("\nğŸ“ˆ å…³é”®æ€§èƒ½æŒ‡æ ‡:")
    for test_name, results in all_results.items():
        if results:
            for operation, data in results.items():
                if isinstance(data, dict) and 'duration' in data:
                    print(f"   {test_name} - {operation}: {data['duration']:.3f}ç§’")
    
    if tests_passed == len(performance_tests):
        print("\nğŸ‰ æ‰€æœ‰æ€§èƒ½æµ‹è¯•é€šè¿‡ï¼ç³»ç»Ÿæ€§èƒ½ä¼˜ç§€ï¼")
        return 0
    elif tests_passed >= len(performance_tests) * 0.8:
        print("\nâš ï¸  å¤§éƒ¨åˆ†æ€§èƒ½æµ‹è¯•é€šè¿‡ï¼Œç³»ç»Ÿæ€§èƒ½è‰¯å¥½")
        return 1
    else:
        print("\nğŸ’¥ å¤šä¸ªæ€§èƒ½æµ‹è¯•å¤±è´¥ï¼Œéœ€è¦ä¼˜åŒ–")
        return 2

if __name__ == "__main__":
    exit_code = main()
    sys.exit(exit_code) 