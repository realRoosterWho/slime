from typing import Optional
import os

from ..abstract_state import AbstractState
from ..derive_states import DeriveState
from ..derive_utils import DeriveChatUtils, encode_image

class ProcessPhotoVoiceState(AbstractState):
    """å¤„ç†æ‹ç…§+è¯­éŸ³æ•°æ®çŠ¶æ€"""
    
    def __init__(self):
        super().__init__(DeriveState.PROCESS_PHOTO_VOICE)
    
    def execute(self, context) -> None:
        """æ‰§è¡Œæ‹ç…§+è¯­éŸ³æ•°æ®å¤„ç†é€»è¾‘"""
        context.logger.log_step("å¤„ç†æ‹ç…§+è¯­éŸ³", "å¼€å§‹å¤„ç†æ‹ç…§æ—¶çš„è¯­éŸ³å’Œç…§ç‰‡æ•°æ®")
        
        try:
            # è·å–æ‹ç…§è¯­éŸ³å’Œç…§ç‰‡æ•°æ®
            photo_voice_text = context.get_data('photo_voice_text')
            photo_path = context.get_data('timestamped_image')
            
            if not photo_voice_text:
                raise ValueError("æ²¡æœ‰æ‰¾åˆ°æ‹ç…§æ—¶çš„è¯­éŸ³æ–‡æœ¬")
            
            context.logger.log_step("æ•°æ®è·å–", f"æ‹ç…§è¯­éŸ³æ–‡æœ¬: {photo_voice_text[:50]}...")
            
            # æ£€æŸ¥æ˜¯å¦æœ‰ç…§ç‰‡ï¼ˆå¦‚æœæ‹ç…§å¤±è´¥å¯èƒ½æ²¡æœ‰ï¼‰
            if photo_path and not context.get_data('photo_failed'):
                # æœ‰ç…§ç‰‡ï¼Œè¿›è¡Œç…§ç‰‡+è¯­éŸ³ç»„åˆåˆ†æ
                self._analyze_photo_with_voice(context, photo_path, photo_voice_text)
            else:
                # ä»…åˆ†æè¯­éŸ³æ–‡æœ¬
                self._analyze_voice_only(context, photo_voice_text)
            
            context.logger.log_step("å¤„ç†æ‹ç…§+è¯­éŸ³", "æ‹ç…§+è¯­éŸ³æ•°æ®å¤„ç†å®Œæˆ")
            
        except Exception as e:
            context.logger.log_step("é”™è¯¯", f"å¤„ç†æ‹ç…§+è¯­éŸ³æ•°æ®å¤±è´¥: {str(e)}")
            # å‘ç”Ÿé”™è¯¯æ—¶ä½¿ç”¨é»˜è®¤å¤„ç†
            self._use_fallback_analysis(context)
    
    def _analyze_photo_with_voice(self, context, photo_path: str, voice_text: str):
        """åˆ†æç…§ç‰‡+è¯­éŸ³ç»„åˆæ•°æ® - å¤šæ­¥éª¤æ–¹æ¡ˆ"""
        context.oled_display.show_text_oled("æ­£åœ¨åˆ†æ\nç…§ç‰‡å’Œè¯­éŸ³...")
        
        try:
            # æ·»åŠ è¯¦ç»†çš„è°ƒè¯•ä¿¡æ¯
            context.logger.log_step("ğŸ” è°ƒè¯•ä¿¡æ¯", f"å¼€å§‹åˆ†æç…§ç‰‡+è¯­éŸ³")
            context.logger.log_step("ğŸ“ ç…§ç‰‡è·¯å¾„", f"åŸå§‹è·¯å¾„: {photo_path}")
            context.logger.log_step("ğŸ—£ï¸ è¯­éŸ³æ–‡æœ¬", f"æ–‡æœ¬é•¿åº¦: {len(voice_text)}, å†…å®¹: {voice_text[:100]}...")
            
            # æ£€æŸ¥ç…§ç‰‡æ–‡ä»¶æ˜¯å¦å­˜åœ¨
            if not os.path.exists(photo_path):
                raise FileNotFoundError(f"ç…§ç‰‡æ–‡ä»¶ä¸å­˜åœ¨: {photo_path}")
            
            # è·å–ç»å¯¹è·¯å¾„
            abs_photo_path = os.path.abspath(photo_path)
            context.logger.log_step("ğŸ“ ç»å¯¹è·¯å¾„", f"ç»å¯¹è·¯å¾„: {abs_photo_path}")
            
            # æ£€æŸ¥æ–‡ä»¶å¤§å°
            file_size = os.path.getsize(photo_path)
            context.logger.log_step("ğŸ“ æ–‡ä»¶å¤§å°", f"æ–‡ä»¶å¤§å°: {file_size} bytes")
            
            if file_size == 0:
                raise ValueError("ç…§ç‰‡æ–‡ä»¶ä¸ºç©º")
            
            # éªŒè¯æ˜¯å¦ä¸ºæœ‰æ•ˆçš„å›¾ç‰‡æ–‡ä»¶
            try:
                from PIL import Image
                with Image.open(photo_path) as img:
                    img_format = img.format
                    img_mode = img.mode  
                    img_size = img.size
                    context.logger.log_step("ğŸ–¼ï¸ å›¾ç‰‡ä¿¡æ¯", f"æ ¼å¼: {img_format}, æ¨¡å¼: {img_mode}, å°ºå¯¸: {img_size}")
            except Exception as e:
                context.logger.log_step("âŒ å›¾ç‰‡éªŒè¯å¤±è´¥", f"ä¸æ˜¯æœ‰æ•ˆçš„å›¾ç‰‡æ–‡ä»¶: {str(e)}")
                raise ValueError(f"æ— æ•ˆçš„å›¾ç‰‡æ–‡ä»¶: {str(e)}")
            
            # ç¼–ç ç…§ç‰‡ä¸ºbase64
            context.logger.log_step("ğŸ”„ å¼€å§‹ç¼–ç ", "å¼€å§‹Base64ç¼–ç ...")
            base64_image = encode_image(photo_path)
            base64_length = len(base64_image)
            context.logger.log_step("âœ… ç¼–ç å®Œæˆ", f"Base64é•¿åº¦: {base64_length} å­—ç¬¦")
            context.logger.log_step("ğŸ”¤ Base64é¢„è§ˆ", f"å‰100å­—ç¬¦: {base64_image[:100]}")
            
            # éªŒè¯base64ç¼–ç æ˜¯å¦æœ‰æ•ˆ
            if base64_length < 100:
                raise ValueError(f"Base64ç¼–ç å¼‚å¸¸çŸ­: {base64_length} å­—ç¬¦")
            
            # æ„å»ºdata URL
            data_url = f"data:image/jpeg;base64,{base64_image}"
            data_url_length = len(data_url)
            context.logger.log_step("ğŸ”— Data URL", f"Data URLæ€»é•¿åº¦: {data_url_length} å­—ç¬¦")
            
            # === ç¬¬ä¸€æ­¥ï¼šç®€å•æè¿°ç…§ç‰‡ï¼ˆé¿å…å†…å®¹è¿‡æ»¤ï¼‰ ===
            context.logger.log_step("ğŸ¯ ç¬¬ä¸€æ­¥", "å¼€å§‹ç®€å•ç…§ç‰‡æè¿°")
            
            chat_utils = DeriveChatUtils(context.response_id)
            
            # ç®€å•æè¿°æç¤ºï¼ˆä¿è¯æˆåŠŸï¼‰
            simple_description_prompt = "è¯·ç®€å•æè¿°è¿™å¼ ç…§ç‰‡ä¸­çœ‹åˆ°çš„å†…å®¹ã€‚"
            
            # æ„å»ºè¾“å…¥å†…å®¹
            input_content = [
                {"type": "input_text", "text": simple_description_prompt},
                {"type": "input_image", "image_url": data_url}
            ]
            
            context.logger.log_step("ğŸ“‹ ç¬¬ä¸€æ­¥è¾“å…¥", f"è¾“å…¥åŒ…å« {len(input_content)} ä¸ªå…ƒç´ ")
            
            # è°ƒç”¨åŸºæœ¬æè¿°
            basic_description = chat_utils.chat_with_continuity(input_content)
            context.response_id = chat_utils.response_id
            
            context.logger.log_step("ğŸ“¨ ç¬¬ä¸€æ­¥ç»“æœ", f"åŸºæœ¬æè¿°: {basic_description}")
            
            # æ£€æŸ¥åŸºæœ¬æè¿°æ˜¯å¦æˆåŠŸ
            failure_keywords = ["æŠ±æ­‰", "æ— æ³•", "ä¸èƒ½", "çœ‹ä¸åˆ°", "æ— æ³•æŸ¥çœ‹", "cannot", "can't", "sorry", "unable"]
            has_failure_keywords = any(keyword in basic_description.lower() for keyword in failure_keywords)
            
            if has_failure_keywords:
                context.logger.log_step("âŒ ç¬¬ä¸€æ­¥å¤±è´¥", "åŸºæœ¬æè¿°è¢«æ‹’ç»ï¼Œå›é€€åˆ°è¯­éŸ³åˆ†æ")
                self._analyze_voice_only(context, voice_text)
                return
            
            # === ç¬¬äºŒæ­¥ï¼šå²è±å§†ä¸ªæ€§åŒ–åˆ†æï¼ˆåŸºäºç¬¬ä¸€æ­¥ç»“æœï¼‰ ===
            context.logger.log_step("ğŸ­ ç¬¬äºŒæ­¥", "å¼€å§‹å²è±å§†ä¸ªæ€§åŒ–åˆ†æ")
            context.oled_display.show_text_oled("å²è±å§†æ­£åœ¨\næ€è€ƒç…§ç‰‡...")
            
            # è·å–å²è±å§†å±æ€§
            slime_obsession = context.get_slime_attribute('obsession', 'å¯»æ‰¾æœ‰è¶£çš„äº‹ç‰©')
            slime_tone = context.get_slime_attribute('tone', 'å‹å¥½å¥½å¥‡')
            slime_quirk = context.get_slime_attribute('quirk', 'å–œæ¬¢æ¢ç´¢')
            
            # å²è±å§†ä¸ªæ€§åŒ–åˆ†ææç¤º
            slime_analysis_prompt = f"""
            ç…§ç‰‡åŸºæœ¬å†…å®¹: {basic_description}
            
            ç”¨æˆ·åŒæ—¶è¯´äº†: "{voice_text}"
            
            ç°åœ¨è¯·ä»å²è±å§†çš„è§’åº¦åˆ†æè¿™ä¸ªåœºæ™¯ï¼š
            
            å²è±å§†çš„æ‰§å¿µ: {slime_obsession}
            å²è±å§†çš„è¯­æ°”: {slime_tone}
            å²è±å§†çš„ç™–å¥½: {slime_quirk}
            
            è¯·ç”Ÿæˆä¸€æ®µåˆ†æï¼ŒåŒ…å«ï¼š
            1. å¯¹ç…§ç‰‡å†…å®¹çš„ä¸ªæ€§åŒ–è§£è¯»
            2. è¿™ä¸ªåœºæ™¯æ˜¯å¦ç¬¦åˆå²è±å§†çš„æ‰§å¿µ
            3. å²è±å§†ä¼šå¦‚ä½•çœ‹å¾…è¿™ä¸ªåœ°æ–¹
            4. ç»“åˆç”¨æˆ·è¯­éŸ³çš„æ•´ä½“æ„Ÿå—
            
            è¯·ç”¨ç”ŸåŠ¨æœ‰è¶£çš„æè¿°ï¼Œæ§åˆ¶åœ¨150å­—ä»¥å†…ã€‚
            """
            
            # è°ƒç”¨å²è±å§†åˆ†æ
            slime_analysis = chat_utils.chat_with_continuity(
                system_content="ä½ æ˜¯ä¸€ä¸ªæœ‰ç‹¬ç‰¹ä¸ªæ€§çš„å²è±å§†ï¼Œä¼šä»è‡ªå·±çš„æ‰§å¿µè§’åº¦æ¥åˆ†æåœºæ™¯ã€‚",
                prompt=slime_analysis_prompt
            )
            context.response_id = chat_utils.response_id
            
            context.logger.log_step("ğŸ­ ç¬¬äºŒæ­¥ç»“æœ", f"å²è±å§†åˆ†æ: {slime_analysis}")
            
            # === ç¬¬ä¸‰æ­¥ï¼šèåˆç”Ÿæˆæœ€ç»ˆåˆ†æ ===
            final_analysis = f"åŸºæœ¬åœºæ™¯: {basic_description}\n\nå²è±å§†çš„çœ‹æ³•: {slime_analysis}"
            
            # ä¿å­˜åˆ†æç»“æœ
            context.set_data('photo_description', final_analysis)
            context.set_data('basic_description', basic_description)  # ä¿å­˜åŸºæœ¬æè¿°å¤‡ç”¨
            context.set_data('slime_analysis', slime_analysis)  # ä¿å­˜å²è±å§†åˆ†æå¤‡ç”¨
            context.set_data('voice_enhanced_analysis', True)  # æ ‡è®°ä¸ºè¯­éŸ³å¢å¼ºåˆ†æ
            
            context.logger.log_step("âœ… å¤šæ­¥éª¤åˆ†ææˆåŠŸ", f"æœ€ç»ˆåˆ†æå·²ç”Ÿæˆ")
            
        except Exception as e:
            context.logger.log_step("âŒ ç…§ç‰‡+è¯­éŸ³åˆ†æé”™è¯¯", f"åˆ†æå¤±è´¥: {str(e)}")
            context.logger.log_step("âŒ é”™è¯¯è¯¦æƒ…", f"é”™è¯¯ç±»å‹: {type(e).__name__}")
            # å›é€€åˆ°ä»…è¯­éŸ³åˆ†æ
            self._analyze_voice_only(context, voice_text)
    
    def _analyze_voice_only(self, context, voice_text: str):
        """ä»…åˆ†æè¯­éŸ³æ–‡æœ¬ï¼ˆå½“æ²¡æœ‰ç…§ç‰‡æ—¶ï¼‰"""
        context.oled_display.show_text_oled("æ­£åœ¨åˆ†æ\nè¯­éŸ³æè¿°...")
        
        try:
            # ä½¿ç”¨èŠå¤©å·¥å…·åˆ†æè¯­éŸ³å†…å®¹
            chat_utils = DeriveChatUtils(context.response_id)
            
            # æ„å»ºè¯­éŸ³åˆ†ææç¤º
            voice_analysis_prompt = f"""
            ç”¨æˆ·åœ¨æŸä¸ªåœºæ™¯ä¸­è¡¨è¾¾äº†è¿™æ ·çš„æ„Ÿå—å’Œæè¿°: "{voice_text}"
            
            è¯·åŸºäºè¿™æ®µè¯åˆ†æ:
            1. ç”¨æˆ·å½“æ—¶æ‰€å¤„çš„ç¯å¢ƒå¯èƒ½æ˜¯ä»€ä¹ˆæ ·çš„
            2. ç”¨æˆ·çš„å¿ƒç†çŠ¶æ€å’Œæƒ…æ„Ÿå€¾å‘
            3. è¿™ä¸ªåœºæ™¯çš„æ°›å›´å’Œç‰¹ç‚¹
            4. é€‚åˆç”Ÿæˆä»€ä¹ˆæ ·çš„å²è±å§†è§’è‰²æ¥åŒ¹é…è¿™ç§æ„Ÿå—
            
            è¯·ç”¨ç”ŸåŠ¨çš„æè¿°å›ç­”ï¼Œæ§åˆ¶åœ¨150å­—ä»¥å†…ã€‚
            """
            
            # è°ƒç”¨åˆ†æ
            voice_analysis = chat_utils.chat_with_continuity(voice_analysis_prompt)
            
            # ä¿å­˜åˆ†æç»“æœ
            context.set_data('photo_description', voice_analysis)
            context.set_data('voice_only_analysis', True)  # æ ‡è®°ä¸ºä»…è¯­éŸ³åˆ†æ
            context.response_id = chat_utils.response_id
            
            context.logger.log_step("è¯­éŸ³åˆ†æ", voice_analysis)
            
        except Exception as e:
            context.logger.log_step("é”™è¯¯", f"è¯­éŸ³åˆ†æå¤±è´¥: {str(e)}")
            self._use_fallback_analysis(context)
    
    def _use_fallback_analysis(self, context):
        """ä½¿ç”¨å¤‡ç”¨åˆ†æï¼ˆå½“æ‰€æœ‰åˆ†æéƒ½å¤±è´¥æ—¶ï¼‰"""
        fallback_description = "è¿™æ˜¯ä¸€ä¸ªå……æ»¡å¯èƒ½æ€§çš„åœ°æ–¹ï¼Œåœ¨è¿™é‡Œå¯ä»¥æ„Ÿå—åˆ°ç‹¬ç‰¹çš„æ°›å›´å’Œç¾å¥½çš„ä½“éªŒã€‚"
        context.set_data('photo_description', fallback_description)
        context.set_data('fallback_analysis', True)
        
        context.logger.log_step("å¤‡ç”¨åˆ†æ", "ä½¿ç”¨é»˜è®¤åœºæ™¯æè¿°")
        
        context.oled_display.show_text_oled(
            "åˆ†æå®Œæˆ\nä½¿ç”¨é»˜è®¤æè¿°\n\næŒ‰BT1ç»§ç»­"
        )
        
        context.oled_display.wait_for_button_with_text(
            context.controller,
            "å‡†å¤‡ç»§ç»­æµç¨‹...",
            context=context
        )
    
    def get_next_state(self, context) -> Optional[DeriveState]:
        """è¿”å›ä¸‹ä¸€ä¸ªçŠ¶æ€"""
        if context.should_return_to_menu():
            return DeriveState.EXIT
        
        # æ­£å¸¸æƒ…å†µä¸‹è¿›å…¥å»ºè®®ç›®çš„åœ°çŠ¶æ€
        return DeriveState.SUGGEST_DESTINATION 