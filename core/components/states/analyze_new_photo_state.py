import base64
from typing import Optional
import os

from ..abstract_state import AbstractState
from ..derive_states import DeriveState
from ..derive_utils import DeriveChatUtils, encode_image

class AnalyzeNewPhotoState(AbstractState):
    """åˆ†ææ–°ç…§ç‰‡çŠ¶æ€ - ç»“åˆç…§ç‰‡å’Œè¯­éŸ³ä¿¡æ¯"""
    
    def __init__(self):
        super().__init__(DeriveState.ANALYZE_NEW_PHOTO)
    
    def execute(self, context) -> None:
        """æ‰§è¡Œåˆ†ææ–°ç…§ç‰‡+è¯­éŸ³é€»è¾‘"""
        context.oled_display.show_text_oled("å²è±å§†æ­£åœ¨\nè§‚å¯Ÿç…§ç‰‡å’Œ\nå¬å–æè¿°...")
        
        try:
            new_photo_path = context.get_data('new_timestamped_image')
            if not new_photo_path:
                raise ValueError("æ²¡æœ‰æ‰¾åˆ°æ–°ç…§ç‰‡è·¯å¾„")
            
            # è·å–è¯­éŸ³æ–‡æœ¬ï¼ˆå¦‚æœæœ‰ï¼‰
            voice_text = context.get_data('new_photo_voice_text', '')
            
            # æ·»åŠ è¯¦ç»†è°ƒè¯•ä¿¡æ¯
            context.logger.log_step("ğŸ” æ–°ç…§ç‰‡åˆ†æè°ƒè¯•", f"å¼€å§‹åˆ†ææ–°ç…§ç‰‡")
            context.logger.log_step("ğŸ“ æ–°ç…§ç‰‡è·¯å¾„", f"è·¯å¾„: {new_photo_path}")
            context.logger.log_step("ğŸ—£ï¸ è¯­éŸ³æ–‡æœ¬", f"é•¿åº¦: {len(voice_text)}, å†…å®¹: {voice_text[:50]}...")
            context.logger.log_step("ğŸ“ æ–‡ä»¶å­˜åœ¨æ£€æŸ¥", f"æ–‡ä»¶å­˜åœ¨: {os.path.exists(new_photo_path)}")
            
            if os.path.exists(new_photo_path):
                file_size = os.path.getsize(new_photo_path)
                context.logger.log_step("ğŸ“ æ–‡ä»¶å¤§å°", f"å¤§å°: {file_size} bytes")
                
                # éªŒè¯å›¾ç‰‡
                try:
                    from PIL import Image
                    with Image.open(new_photo_path) as img:
                        context.logger.log_step("ğŸ–¼ï¸ å›¾ç‰‡éªŒè¯", f"æ ¼å¼: {img.format}, å°ºå¯¸: {img.size}")
                except Exception as e:
                    context.logger.log_step("âŒ å›¾ç‰‡éªŒè¯å¤±è´¥", f"é”™è¯¯: {str(e)}")
                    raise ValueError(f"æ— æ•ˆçš„å›¾ç‰‡æ–‡ä»¶: {str(e)}")
            
            # ç¼–ç å›¾ç‰‡ä¸ºbase64
            context.logger.log_step("ğŸ”„ Base64ç¼–ç ", "å¼€å§‹ç¼–ç ...")
            base64_image = encode_image(new_photo_path)
            context.logger.log_step("âœ… ç¼–ç å®Œæˆ", f"é•¿åº¦: {len(base64_image)} å­—ç¬¦")
            
            data_url = f"data:image/jpeg;base64,{base64_image}"
            context.logger.log_step("ğŸ”— Data URL", f"æ€»é•¿åº¦: {len(data_url)} å­—ç¬¦")
            
            context.logger.log_step("åˆ†ææ–°ç…§ç‰‡+è¯­éŸ³", f"å¼€å§‹åˆ†ææ–°ç…§ç‰‡: {new_photo_path}, è¯­éŸ³é•¿åº¦: {len(voice_text)}")
            
            # è·å–å²è±å§†çš„æ‰§å¿µå’Œå±æ€§
            slime_obsession = context.get_slime_attribute('obsession')
            slime_tone = context.get_slime_attribute('tone')
            
            context.logger.log_step("ğŸ¤– å²è±å§†å±æ€§", f"æ‰§å¿µ: {slime_obsession}, è¯­æ°”: {slime_tone}")
            
            # ä½¿ç”¨èŠå¤©å·¥å…·åˆ†æç…§ç‰‡å’Œè¯­éŸ³
            chat_utils = DeriveChatUtils(context.response_id)
            
            # æ„å»ºç»¼åˆåˆ†ææç¤º
            if voice_text and len(voice_text.strip()) > 0:
                analysis_text = f"""
                è¯·æè¿°è¿™å¼ ç…§ç‰‡çš„å†…å®¹ã€‚
                
                ç”¨æˆ·è¯´äº†: "{voice_text}"
                
                è¯·æè¿°ç…§ç‰‡ä¸­çœ‹åˆ°çš„å†…å®¹ï¼Œå¹¶ç»“åˆç”¨æˆ·çš„æè¿°ç»™å‡ºæ•´ä½“å°è±¡ã€‚
                """
            else:
                analysis_text = f"""
                è¯·æè¿°è¿™å¼ ç…§ç‰‡çš„å†…å®¹ï¼ŒåŒ…æ‹¬çœ‹åˆ°çš„ç‰©ä½“ã€ç¯å¢ƒã€æ°›å›´ç­‰ã€‚
                """
            
            # æ„å»ºåŒ…å«å›¾ç‰‡å’Œæ–‡æœ¬çš„è¾“å…¥
            input_content = [
                {"type": "input_text", "text": analysis_text},
                {"type": "input_image", "image_url": data_url}
            ]
            
            context.logger.log_step("ğŸ“ æç¤ºè¯", f"æç¤ºè¯é•¿åº¦: {len(analysis_text)}")
            context.logger.log_step("ğŸ“‹ è¾“å…¥æ ¼å¼", f"è¾“å…¥åŒ…å« {len(input_content)} ä¸ªå…ƒç´ ")
            context.logger.log_step("ğŸ“‹ è¾“å…¥ç»“æ„", f"å…ƒç´ 1ç±»å‹: {input_content[0]['type']}, å…ƒç´ 2ç±»å‹: {input_content[1]['type']}")
            context.logger.log_step("ğŸ¤– å‘é€è¯·æ±‚", "å‘é€æ–°ç…§ç‰‡åˆ†æåˆ°OpenAI...")
            
            response = chat_utils.chat_with_continuity(input_content)
            context.response_id = chat_utils.response_id
            
            context.logger.log_step("ğŸ“¨ AIå›å¤", f"å›å¤é•¿åº¦: {len(response)}")
            context.logger.log_step("ğŸ“¨ AIå›å¤å†…å®¹", f"å®Œæ•´å›å¤: {response}")
            
            # æ£€æŸ¥å›å¤è´¨é‡
            failure_keywords = ["æŠ±æ­‰", "æ— æ³•", "ä¸èƒ½", "çœ‹ä¸åˆ°", "æ— æ³•æŸ¥çœ‹", "cannot", "can't", "sorry", "unable"]
            success_keywords = ["çœ‹åˆ°", "å›¾ç‰‡", "ç…§ç‰‡", "ç”»é¢", "ç”»ä¸­", "see", "image", "photo"]
            
            has_failure_keywords = any(keyword in response.lower() for keyword in failure_keywords)
            has_success_keywords = any(keyword in response.lower() for keyword in success_keywords)
            
            context.logger.log_step("ğŸ” å…³é”®è¯åˆ†æ", f"å¤±è´¥å…³é”®è¯: {has_failure_keywords}, æˆåŠŸå…³é”®è¯: {has_success_keywords}")
            
            # ä¿å­˜æ–°ç…§ç‰‡åˆ†æç»“æœ
            context.set_data('new_photo_analysis', response)
            
            # è®°å½•åˆ†æç»“æœ
            analysis_type = "ç…§ç‰‡+è¯­éŸ³åˆ†æ" if voice_text else "ç…§ç‰‡åˆ†æ"
            context.logger.log_step(f"åˆ†ææ–°{analysis_type}", f"åˆ†æå®Œæˆ: {response[:50]}...")
            
            # æ˜¾ç¤ºåˆ†æç»“æœï¼ŒåŒ…å«è¯­éŸ³ä¿¡æ¯
            if voice_text and len(voice_text.strip()) > 0:
                display_text = f"å²è±å§†å¬äº†ä½ çš„è¯\nçœ‹äº†ç…§ç‰‡åè¯´ï¼š\n{response[:60]}..."
            else:
                display_text = f"å²è±å§†çœ‹äº†çœ‹ï¼š\n{response[:80]}..."
                
            result = context.oled_display.wait_for_button_with_text(
                context.controller,
                display_text,
                context=context
            )
            
            # æ£€æŸ¥æ˜¯å¦æ˜¯é•¿æŒ‰è¿”å›èœå•
            if result == 2:
                context.logger.log_step("ç”¨æˆ·æ“ä½œ", "ç”¨æˆ·é•¿æŒ‰æŒ‰é’®2è¿”å›èœå•")
                return
            
        except Exception as e:
            context.logger.log_step("é”™è¯¯", f"åˆ†ææ–°ç…§ç‰‡+è¯­éŸ³å¤±è´¥: {str(e)}")
            
            # è®¾ç½®é»˜è®¤åˆ†æç»“æœ
            voice_text = context.get_data('new_photo_voice_text', '')
            if voice_text:
                default_analysis = f"å¬äº†ä½ çš„æè¿°ï¼Œçœ‹èµ·æ¥å¾ˆæœ‰æ„æ€ï¼è®©æˆ‘æƒ³æƒ³è¿™èƒ½å¸¦æ¥ä»€ä¹ˆå¥–åŠ±..."
            else:
                default_analysis = "çœ‹èµ·æ¥å¾ˆæœ‰è¶£ï¼Œè®©æˆ‘æƒ³æƒ³è¿™èƒ½å¸¦æ¥ä»€ä¹ˆå¥–åŠ±..."
                
            context.set_data('new_photo_analysis', default_analysis)
            
            # æ˜¾ç¤ºé»˜è®¤åˆ†æç»“æœ
            result = context.oled_display.wait_for_button_with_text(
                context.controller,
                f"å²è±å§†çœ‹äº†çœ‹ï¼š\n{default_analysis}",
                context=context
            )
            
            # æ£€æŸ¥æ˜¯å¦æ˜¯é•¿æŒ‰è¿”å›èœå•
            if result == 2:
                context.logger.log_step("ç”¨æˆ·æ“ä½œ", "ç”¨æˆ·é•¿æŒ‰æŒ‰é’®2è¿”å›èœå•")
    
    def get_next_state(self, context) -> Optional[DeriveState]:
        """è¿”å›ä¸‹ä¸€ä¸ªçŠ¶æ€"""
        if context.should_return_to_menu():
            return DeriveState.EXIT
        
        return DeriveState.ANALYZE_REWARD 